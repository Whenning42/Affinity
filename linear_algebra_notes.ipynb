{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNRXabPghpG9Vtc1D0IV8jJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Whenning42/Affinity/blob/master/linear_algebra_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Williamâ€™s Linear Algebra Notes\n",
        "\n",
        "## Latex Background\n",
        "\n",
        "An latex command is given between two `$` symbols. For example `$m \\times n$` renders as $m \\times n$.\n",
        "\n",
        "Here's a table of some common notation:\n",
        "\n",
        "| Name  | Rendered | Latex |\n",
        "|-------|--------------|------------|\n",
        "| Reals | $\\mathbb{R}$ | \\mathbb{R} |\n",
        "| Complex | $\\mathbb{C}$ | \\mathbb{C} |\n",
        "| Exponentiation | $a^b$, $a^{b+c}$ | a^b, a^{b+c}, note to put more than a letter in the exponent, we use curly brackets. |\n",
        "| Transpose | $A^\\top$ | A^\\top |\n",
        "| Dot Product | $x \\cdot y$ | x \\cdot y |"
      ],
      "metadata": {
        "id": "1dEkCUUkXx8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrices\n",
        "\n",
        "A matrix is an $m \\times n$ rectangle consisting of $m$ rows and $n$ columns of scalar values from some field $\\mathbb{F}$ (commonly $\\mathbb{R}$ or $\\mathbb{C}$). We deonte the space of $m \\times n$ matrices as $\\mathbb{F}^{m \\times n}$.\n",
        "\n",
        "An example $2 \\times 3$ matrix $A$ over the reals is:\n",
        "$A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$\n",
        "\n",
        "### Latex\n",
        "\n",
        "We use $\\text{\\begin{bmatrix}}$ and $\\text{\\end{bmatrix}}$ to create a bracketted matrix. So to render the above matrix, we write:\n",
        "\n",
        "```\n",
        "A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\n",
        "```"
      ],
      "metadata": {
        "id": "vTlqGWEKgk0M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0I7d0NsXwX-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ea7efd-77e8-438e-d873-980ba739f46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "[[1 2 3]\n",
            " [4 5 6]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# Note: All libraries use row major notation.\n",
        "# Sets a to:\n",
        "# [[1, 2, 3]\n",
        "#  [4, 5, 6]]\n",
        "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "print(a)\n",
        "\n",
        "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(a)\n",
        "\n",
        "a = jnp.array([[1, 2, 3], [4, 5, 6]])\n",
        "print(a)\n",
        "\n",
        "# TODO: Add some simple matrix constructors like eye or diag."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block Matrices\n",
        "\n",
        "A block matrix is a matrix that concatenates a rectangular grid of submatrices. Block matrices are denoted as:\n",
        "\n",
        "$M = \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix}$\n",
        "\n",
        "With concatenated matrices needing to match the size of their neighbors along their adjacent sides."
      ],
      "metadata": {
        "id": "ioW-9mRGiRLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy\n",
        "a = np.array([[2]])\n",
        "b = np.array([[4, 6]])\n",
        "c = np.array([[8]])\n",
        "d = np.array([[10, 12]])\n",
        "M = np.block([[a, b], [c, d]])\n",
        "print(M)\n",
        "\n",
        "# Torch\n",
        "#\n",
        "# Torch doesn't have a block matrix constructor.\n",
        "# We can however use `torch.cat().\n",
        "a = torch.tensor(a)\n",
        "b = torch.tensor(b)\n",
        "c = torch.tensor(c)\n",
        "d = torch.tensor(d)\n",
        "t = torch.cat([a, b], dim=1)\n",
        "b = torch.cat([c, d], dim=1)\n",
        "M = torch.cat([t, b], dim = 0)\n",
        "print(M)\n",
        "\n",
        "# Jax\n",
        "a = np.array([[2]])\n",
        "b = np.array([[4, 6]])\n",
        "c = np.array([[8]])\n",
        "d = np.array([[10, 12]])\n",
        "M = jnp.block([[a, b], [c, d]])\n",
        "print(M)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhVe1caWiqZx",
        "outputId": "661710a6-b996-4487-ce79-8bbbde91c3a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2  4  6]\n",
            " [ 8 10 12]]\n",
            "tensor([[ 2,  4,  6],\n",
            "        [ 8, 10, 12]])\n",
            "[[ 2  4  6]\n",
            " [ 8 10 12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Transpose\n",
        "\n",
        "The transpose of a matrix $A$ is denoted as $A^\\top$ and is the result of flipping that matrix across its diagonal.\n",
        "\n",
        "Formally $A^\\top$ is defined such that $A^\\top_{j,i} = A_{i,j} \\ \\forall i, j$.\n",
        "\n",
        "### Latex\n",
        "\n",
        "We use `^\\top` to indicate the transpose, so for example `A^\\top` gives us $A^\\top$."
      ],
      "metadata": {
        "id": "LWSDAG7_kQ31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A transpose can be applied in both the Numpy and Torch APIs either\n",
        "# by reordering axes or swapping two axes. We show both options for\n",
        "# both APIs. Do note the APIs use different names for these operations.\n",
        "\n",
        "# Numpy\n",
        "# `np.transpose` reorders axes.\n",
        "# `np.swapaxes()` swaps two axes.\n",
        "a = np.array([[1, 2]])\n",
        "print(np.transpose(a, axes=(1, 0)))\n",
        "print(np.swapaxes(a, 0, 1))\n",
        "\n",
        "\n",
        "# Torch\n",
        "# `torch.permute` reorders axes.\n",
        "# `torch.swapaxes()` swaps two axes.\n",
        "a = torch.Tensor([[1,  2]])\n",
        "print(torch.permute(a, (1, 0)))\n",
        "print(torch.transpose(a, 0, 1))\n",
        "\n",
        "# Jax\n",
        "a = jnp.array([[1, 2]])\n",
        "print(jnp.transpose(a, (1, 0)))\n",
        "print(jnp.swapaxes(a, 0, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb1_bSz7mx8G",
        "outputId": "3bac544f-b4c2-4638-bdfb-52883bf0d851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The transpose of [[1 2]] is:\n",
            "[[1]\n",
            " [2]]\n",
            "[[1]\n",
            " [2]]\n",
            "tensor([[1.],\n",
            "        [2.]])\n",
            "tensor([[1.],\n",
            "        [2.]])\n",
            "[[1]\n",
            " [2]]\n",
            "[[1]\n",
            " [2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Multiplication\n",
        "\n",
        "The product matrices $A$ and $B$ of size $m \\times n$ and $n \\times k$ respectively is denoted $AB$ and it's a matrix of size $m \\times k$ where:\n",
        "\n",
        "$AB_{ij} = A_i \\cdot B_{:,j}$"
      ],
      "metadata": {
        "id": "3K9jMPTV2jjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: Both numpy and torch use the `*` operator on matrices to be an\n",
        "# elementwise product, not a matrix multiply.\n",
        "a = np.array([[1, 5], [2, 6]])\n",
        "b = np.array([[1, 0], [0, 2]])\n",
        "print(\"A\\n\", a)\n",
        "print(\"B\\n\", b)\n",
        "print(\"AB\")\n",
        "\n",
        "# Numpy\n",
        "# A matrix multiply can be performed with `np.matmul()`, the `@` operator, or an\n",
        "# einsum.\n",
        "print(a @ b)\n",
        "print(np.matmul(a, b))\n",
        "print(np.einsum('ij,jk->ik', a, b))\n",
        "\n",
        "# Torch\n",
        "a = torch.tensor(a)\n",
        "b = torch.tensor(b)\n",
        "print(a @ b)\n",
        "print(torch.matmul(a, b))\n",
        "print(torch.einsum('ij,jk->ik', a, b))\n",
        "\n",
        "# Do note that Numpy's einsum is much slower that matmul since it's a python\n",
        "# implementation whereas Torch and Jax's einsum are close in speed to\n",
        "# their matmuls.\n",
        "a = np.ones((500, 500))\n",
        "b = np.ones((500, 500))\n",
        "print(\"Numpy matmul vs einsum:\")\n",
        "%timeit np.matmul(a, b)\n",
        "%timeit np.einsum('ij,jk->ik', a, b)\n",
        "a = torch.tensor(a)\n",
        "b = torch.tensor(b)\n",
        "print(\"Torch matmul vs einsum:\")\n",
        "%timeit torch.matmul(a, b)\n",
        "%timeit torch.einsum('ij,jk->ik', a, b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaEcO2GB5nwc",
        "outputId": "bf253e5c-b7e7-44a5-8f4a-e7f2f457af87"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            " [[1 5]\n",
            " [2 6]]\n",
            "B\n",
            " [[1 0]\n",
            " [0 2]]\n",
            "AB\n",
            "[[ 1 10]\n",
            " [ 2 12]]\n",
            "[[ 1 10]\n",
            " [ 2 12]]\n",
            "[[ 1 10]\n",
            " [ 2 12]]\n",
            "tensor([[ 1, 10],\n",
            "        [ 2, 12]])\n",
            "tensor([[ 1, 10],\n",
            "        [ 2, 12]])\n",
            "tensor([[ 1, 10],\n",
            "        [ 2, 12]])\n",
            "Numpy matmul vs einsum:\n",
            "11.3 ms Â± 2.63 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n",
            "134 ms Â± 40.7 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
            "Torch matmul vs einsum:\n",
            "18.8 ms Â± 4.49 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
            "9.95 ms Â± 2.41 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "- add batching to our examples?\n",
        "  - I'm not sure which of the functions in this notebook would might be non-obvious in terms of how they generalize to batched matrices.\n",
        "- vec op\n",
        "- invertability\n",
        "- orthonormal matrices\n",
        "- angle between vectors\n",
        "- determinant\n",
        "- eigenvalues\n",
        "- eigenvectors\n",
        "- trace\n",
        "- chapter 2 exercises?"
      ],
      "metadata": {
        "id": "VCfxwaTv7rO1"
      }
    }
  ]
}